# ğŸ“˜ Simple GenAI App â€” LangChain + FAISS + Groq Llama

This project is a Simple GenAI Retrieval-Augmented Generation (RAG) App built using:

LangChain

Web scraping + text processing

HuggingFace embeddings

FAISS vector database

Groq Llama-3.1 (fast inference)

The notebook demonstrates the full GenAI workflow from data ingestion â†’ vectorization â†’ retrieval â†’ LLM answering using LangChainâ€™s modern components.

## ğŸš€ What This App Does

âœ” Scrapes documentation from a live webpage

âœ” Cleans & loads page content into LangChain Document format

âœ” Splits content into overlapping chunks

âœ” Converts text chunks into dense HuggingFace embeddings

âœ” Stores vectors in a FAISS vector database

âœ” Retrieves relevant chunks based on a user query

âœ” Uses Groq Llama-3.1 to answer questions purely from retrieved context

âœ” Demonstrates an end-to-end RAG pipeline inside a single notebook


## ğŸ› ï¸ Tech Stack
ğŸ”¹ Python + Jupyter Notebook

Main development environment.

ğŸ”¹ LangChain

WebBaseLoader for scraping

RecursiveCharacterTextSplitter for chunking

create_retrieval_chain and create_stuff_documents_chain

Modern Runnable architecture

ğŸ”¹ Embeddings

sentence-transformers/all-mpnet-base-v2 via HuggingFace

ğŸ”¹ Vector Store

FAISS (fast, lightweight, in-memory ANN search)

ğŸ”¹ LLM (Groq)

llama-3.1-8b-instant (super-fast inference)

Optional: llama-3.1-70b-versatile

## ğŸ“‚ Project Structure
ğŸ“ simple-genai-app/
 â”œâ”€â”€ SimpleGenAIapp.ipynb   # Full RAG pipeline notebook
 â””â”€â”€ README.md              # Project documentation

## â–¶ï¸ How to Run the Notebook
1. Clone the repository

```bash
git clone https://github.com/whatsupwithdeepti/simple-genai-app.git
cd simple-genai-app 
```

2. Install dependencies

3. Add environment variables

Create a .env file:

HF_TOKEN=your_huggingface_token

LANGCHAIN_API_KEY=your_langsmith_key

LANGCHAIN_PROJECT=your_project_name

GROQ_API_KEY=your_groq_key

4. Run the notebook
jupyter notebook SimpleGenAIapp.ipynb

## ğŸ§  Pipeline Overview
1ï¸âƒ£ Data Ingestion
loader = WebBaseLoader(url)
documents = loader.load()

2ï¸âƒ£ Chunking
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)

3ï¸âƒ£ Embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

4ï¸âƒ£ Vector DB
vectorstore = FAISS.from_documents(chunks, embeddings)

5ï¸âƒ£ Retrieval Chain + LLM
retriever = vectorstore.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)

6ï¸âƒ£ Ask a Question
response = retrieval_chain.invoke({"input": "Your question here"})

## ğŸ“ Example Question

"LangSmith has two usage limits: what are they?"

The app retrieves relevant documentation, processes it, and generates an LLM-based answer only using the retrieved context.

## ğŸ“Œ Future Improvements

Add Streamlit / Gradio UI

Store FAISS index locally for persistence

Add multi-page web scraping

Add reranking using BGE or Cohere reranker

Add evaluation metrics

Build this into a full RAG microservice

## ğŸ¤ Contributing

Feel free to fork and improve the project. PRs are welcome!

## â­ Author

Built by Deepti Jethwani
A beginner-friendly GenAI RAG notebook for learning LangChain, FAISS, and Groq LLMs.
